{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cbc180-f969-45e3-b6e3-81be49c2f2ff",
   "metadata": {},
   "source": [
    "Name : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876309c-c5df-4e39-98aa-800c5197e790",
   "metadata": {},
   "source": [
    "E-mail : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64bcbe-63ae-4cd4-b37c-7a20e29336c1",
   "metadata": {},
   "source": [
    "# Useful Jupyter Notebook Shortcuts\n",
    "\n",
    "Here are some helpful keyboard shortcuts for Jupyter Notebook:\n",
    "\n",
    "- **M**: Switch to Markdown mode\n",
    "- **Y**: Switch to Code mode\n",
    "- **A**: Insert cell above\n",
    "- **B**: Insert cell below\n",
    "- **D, D**: (Press D twice) Delete selected cell\n",
    "- **Shift + Enter**: Run the current cell and move to the next\n",
    "- **Ctrl + Enter**: Run the current cell and stay on it\n",
    "- **Shift + Tab**: Show function/method documentation\n",
    "- **Ctrl + Shift + -**: Split cell at cursor\n",
    "- **Esc**: Enter command mode (blue border)\n",
    "- **Enter**: Enter edit mode (green border)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916aef7-e133-4c5c-9a99-c7acd109af62",
   "metadata": {},
   "source": [
    "## default imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e019e3cf-3c57-4107-a16c-03cda08c56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for internal use\n",
    "from urllib.error import URLError\n",
    "from tensorflow.data import Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from keras.utils import image_dataset_from_directory, get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89fc4af-d66a-4efd-bb98-2da918967a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3026b0ef-d34a-413f-8647-d01c8037c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis import\n",
    "from skimage import io, img_as_float\n",
    "from scipy import stats\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb0253a-f0d7-4901-b5ca-8efa26a4882b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# machine learning imports\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.layers import Input, Dense, Conv1D, Conv2D, UpSampling2D, MaxPooling2D, AveragePooling2D, Rescaling, Activation, Add, Flatten, Reshape\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers, regularizers\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96eaa1a-13aa-4427-b278-64bb274364bf",
   "metadata": {},
   "source": [
    "## internal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a346daa-ab25-4bd4-afef-560de52df528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import tensorflow as tf\n",
    "\n",
    "def transpose_image_grayscale(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Transpose a grayscale image from portrait to landscape orientation if necessary.\n",
    "\n",
    "    This function checks the dimensions of the input image and transposes it if\n",
    "    the height (first dimension) is less than the width (second dimension).\n",
    "    The transposition ensures the image is always in landscape orientation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : tf.Tensor\n",
    "        A 3D tensor representing a grayscale image with shape (height, width, 1).\n",
    "        The last dimension represents the single color channel.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        A 3D tensor with the same content as the input image, but guaranteed to be\n",
    "        in landscape orientation (width >= height).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function assumes the input is a valid TensorFlow tensor representing\n",
    "    a grayscale image with three dimensions (height, width, channel).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import tensorflow as tf\n",
    "    >>> # Create a portrait grayscale image (5x3x1)\n",
    "    >>> portrait_image = tf.random.normal([5, 3, 1])\n",
    "    >>> landscape_result = transpose_image_grayscale(portrait_image)\n",
    "    >>> print(tf.shape(landscape_result).numpy())\n",
    "    [3 5 1]\n",
    "\n",
    "    >>> # Create a landscape grayscale image (3x5x1)\n",
    "    >>> landscape_image = tf.random.normal([3, 5, 1])\n",
    "    >>> result = transpose_image_grayscale(landscape_image)\n",
    "    >>> print(tf.shape(result).numpy())\n",
    "    [3 5 1]\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    tf.transpose : The underlying TensorFlow operation used for transposition.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input tensor does not have exactly 3 dimensions.\n",
    "    \"\"\"\n",
    "    # Check if image is in portrait mode \n",
    "    if tf.shape(image)[0] < tf.shape(image)[1]:\n",
    "        # Transpose to landscape mode \n",
    "        return tf.transpose(image, perm=[1, 0, 2])  # No need for channel dim in grayscale\n",
    "    else:\n",
    "        # If it's already landscape, return the image as is\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4e082-c320-4205-9bb8-e4b2476f4681",
   "metadata": {},
   "source": [
    "## Public Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4866ea5b-fed6-492a-b49c-58f771b4a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_create_datasets(\n",
    "    url: str,\n",
    "    fname: str,\n",
    "    batch_size: int = 16,\n",
    "    image_size: tuple[int, int] = (480, 320)\n",
    ") -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Load and prepare image datasets for denoising tasks.\n",
    "\n",
    "    This function downloads a dataset, extracts it, and creates TensorFlow datasets\n",
    "    for training and testing. It handles both original (clean) and noisy images,\n",
    "    applying necessary preprocessing steps including transposition and normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL or local path to the dataset zip file.\n",
    "    fname : str\n",
    "        Filename to save the downloaded dataset.\n",
    "    batch_size : int, optional\n",
    "        Batch size for the datasets, by default 16.\n",
    "    image_size : tuple of int, optional\n",
    "        Target size for the images (height, width), by default (480, 320).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dataset, Dataset]\n",
    "        A tuple containing:\n",
    "        - dataset_train: A TensorFlow Dataset containing pairs of (noisy, original) images for training\n",
    "        - dataset_test: A TensorFlow Dataset containing pairs of (noisy, original) images for testing\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function performs several steps:\n",
    "    1. Downloads and extracts the dataset\n",
    "    2. Creates separate datasets for original and noisy images, both for training and testing\n",
    "    3. Applies image transposition to ensure consistent orientation\n",
    "    4. Normalizes pixel values from [0, 255] to [0, 1]\n",
    "\n",
    "    The expected dataset structure after extraction is:\n",
    "    - BSD400/original/ (training original images)\n",
    "    - BSD400/noise_gaussian_15/ (training noisy images)\n",
    "    - BSD68/original/ (testing original images)\n",
    "    - BSD68/noise_gaussian_15/ (testing noisy images)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> url = \"https://example.com/dataset.zip\"\n",
    "    >>> fname = \"denoising_dataset.zip\"\n",
    "    >>> train_ds, test_ds = load_and_create_datasets(url, fname)\n",
    "    >>> for noisy_batch, clean_batch in train_ds.take(1):\n",
    "    ...     print(f\"Batch shape: {noisy_batch.shape}\")\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If the URL download fails, it attempts to use the URL as a local filename.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - All images are loaded in grayscale mode\n",
    "    - Images are automatically transposed if needed using the transpose_image_grayscale function\n",
    "    - Pixel values are rescaled from 0-255 to 0-1\n",
    "    - The function uses a specific file hash for verification\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    transpose_image_grayscale : Function used for ensuring consistent image orientation\n",
    "    tensorflow.keras.utils.get_file : Used for downloading and extracting the dataset\n",
    "    \"\"\"\n",
    "    image_size = tuple(image_size[0:2])\n",
    "    dset_name = Path(fname).stem\n",
    "\n",
    "    FileHash = r\"d68e70fa5e9ba1ae4d36dd40a0095f1e1bb49d6574c0372eb5079c636fce0651\"\n",
    "    \n",
    "    try:\n",
    "        dset_download = get_file(fname=fname,\n",
    "            origin=url,\n",
    "            extract=True,\n",
    "            file_hash=FileHash,\n",
    "            archive_format=\"zip\",\n",
    "            force_download=False\n",
    "        )\n",
    "    except Exception:\n",
    "            print(\"URL download failed, try using DATASET_URL as a local filename\")\n",
    "            dset_download = get_file(fname=fname,\n",
    "                                          origin  = \"file:\\\\\"+url,\n",
    "            extract=True,\n",
    "            file_hash=FileHash,\n",
    "            archive_format=\"zip\",\n",
    "            force_download=False\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Step 2: Get the path of the extracted directory\n",
    "    dataset_dir = os.path.dirname(dset_download)\n",
    "    image_dataset_from_directory_common_args = {\n",
    "        \"label_mode\":None,\n",
    "        \"class_names\":None,\n",
    "        \"color_mode\":\"grayscale\",\n",
    "        \"batch_size\":batch_size,\n",
    "        \"image_size\":image_size,\n",
    "        \"shuffle\":False,\n",
    "        \"seed\":42,\n",
    "        \"validation_split\":None,\n",
    "        \"subset\":None,\n",
    "        \"interpolation\":\"bilinear\",\n",
    "        \"follow_links\":False,\n",
    "        \"crop_to_aspect_ratio\":False,\n",
    "        \"pad_to_aspect_ratio\":False,\n",
    "        \"data_format\":None,\n",
    "        \"verbose\":False,\n",
    "    }\n",
    "    \n",
    "    train_images_original = image_dataset_from_directory(\n",
    "        Path(dataset_dir).joinpath(dset_name, \"BSD400\",\"original\"),\n",
    "        **image_dataset_from_directory_common_args\n",
    "    )\n",
    "    \n",
    "    train_images_noisy = image_dataset_from_directory(\n",
    "        Path(dataset_dir).joinpath(dset_name, \"BSD400\",\"noise_gaussian_15\"),\n",
    "        **image_dataset_from_directory_common_args\n",
    "    )\n",
    "    \n",
    "    test_images_original = image_dataset_from_directory(\n",
    "        Path(dataset_dir).joinpath(dset_name, \"BSD68\", \"original\"),\n",
    "        **image_dataset_from_directory_common_args\n",
    "    )\n",
    "    \n",
    "    test_images_noisy = image_dataset_from_directory(\n",
    "        Path(dataset_dir).joinpath(dset_name, \"BSD68\", \"noise_gaussian_15\"),\n",
    "        **image_dataset_from_directory_common_args\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Apply the transpose function for grayscale images\n",
    "    train_images_original = train_images_original.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
    "    test_images_original = test_images_original.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
    "    train_images_noisy = train_images_noisy.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
    "    test_images_noisy = test_images_noisy.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
    "    \n",
    "    \n",
    "    rescale = Rescaling(1.0 / 255)\n",
    "    \n",
    "    # Apply the Rescaling layer for normalization to both datasets\n",
    "    train_images_original = train_images_original.map(lambda x: rescale(x))\n",
    "    test_images_original = test_images_original.map(lambda x: rescale(x))\n",
    "    train_images_noisy = train_images_noisy.map(lambda x: rescale(x))\n",
    "    test_images_noisy = test_images_noisy.map(lambda x: rescale(x))\n",
    "    \n",
    "    \n",
    "    dataset_train = Dataset.zip((train_images_noisy, train_images_original))\n",
    "    dataset_test = Dataset.zip((test_images_noisy, test_images_original))\n",
    "    print(\"Datasets for train and test, created. Please note that pixels values have been rescale from 0->255 to 0->1\")\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e499907-4247-4e24-a055-f5047ca615cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_list(dataset):\n",
    "    flat_images_noisy = []\n",
    "    flat_images_original = []\n",
    "    \n",
    "    for noisy_batch, original_batch in dataset:\n",
    "        # Convert to numpy and flatten the batch dimension\n",
    "        noisy_images = noisy_batch.numpy()\n",
    "        original_images = original_batch.numpy()\n",
    "        \n",
    "        # Extend our lists with the flattened batches\n",
    "        flat_images_noisy.extend(noisy_images)\n",
    "        flat_images_original.extend(original_images)\n",
    "    \n",
    "    return tuple([x.squeeze() for x in flat_images_noisy]), tuple([x.squeeze() for x in flat_images_original])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eee9131-9723-486d-b0a3-b45051c42fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def fourier_denoise(image, threshold=0.1):\n",
    "       # Compute the 2D FFT of the image\n",
    "       f = np.fft.fft2(image)\n",
    "       \n",
    "       # Shift the zero-frequency component to the center\n",
    "       f_shift = np.fft.fftshift(f)\n",
    "       \n",
    "       # Create a mask based on the threshold\n",
    "       mask = np.abs(f_shift) > threshold * np.max(np.abs(f_shift))\n",
    "       \n",
    "       # Apply the mask\n",
    "       f_shift_filtered = f_shift * mask\n",
    "       \n",
    "       # Shift back\n",
    "       f_filtered = np.fft.ifftshift(f_shift_filtered)\n",
    "       \n",
    "       # Compute the inverse 2D FFT\n",
    "       denoised = np.real(np.fft.ifft2(f_filtered))\n",
    "       \n",
    "       return np.clip(denoised, 0., 1.), f_shift, f_shift_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9825a62-9707-44b4-abcd-43c8e8786e35",
   "metadata": {},
   "source": [
    "## global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b41895-e41f-44a8-abad-f277322e0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = \"https://amubox.univ-amu.fr/s/TP3mLFKikYdxt7o/download/dataset_bsd400_68.zip\"\n",
    "DATASET_FNAME = \"dataset_bsd400_68.zip\"\n",
    "# images downsampled to reduce memory usage during training\n",
    "IMAGE_SHAPE = (240, 160, 1) # last value correspond to the number of channels : 1 for grayscale, 3 for rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33409a-1448-4b70-9caa-d096894fb2c0",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af743f9a-ae8c-410b-a55a-1d1836eb7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets for train and test, created. Please note that pixels values have been rescale from 0->255 to 0->1\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_test = load_and_create_datasets(DATASET_URL, DATASET_FNAME, image_size=IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1515939e-b2c3-42c5-9e0c-a7199d3922df",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_noise_test, images_ref_test = dataset_to_list(dataset_test)\n",
    "images_noise_train, images_ref_train = dataset_to_list(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60b451-5280-4a78-833e-85e4d8453e7c",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a7aab-e4b9-4ffd-9a16-f6f8a89346c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc6a55-c222-47e8-b3ae-455669f837f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2d1ae4f-3f16-4007-8be3-01aba2730fc7",
   "metadata": {},
   "source": [
    "## Perceptron Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958390a7-97a4-4b02-9776-d532801c4f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d4110-5b06-4716-b7a7-ef26291e1a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06233117-2523-4711-b72b-d82af0640d4d",
   "metadata": {},
   "source": [
    "## CNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb47a7c-d27c-47ef-aeb4-71a9be3ef223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d7531e-b828-4a72-9cbc-429f80de028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37059e63-8522-4e83-b659-301d5119227f",
   "metadata": {},
   "source": [
    "## Conclustions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b778c7c-ed23-4477-b0e8-bfbb7ed20aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0c7c7-7f30-44b7-9986-a6691a4db807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
